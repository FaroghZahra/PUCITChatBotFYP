{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EwD6xD-3GRL"
      },
      "source": [
        "## NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gngjpi701nlP"
      },
      "source": [
        "Natural Language Processing (NLP) is the technology used to help machines to understand and learn text and language. With NLP data scientists aim to teach machines to understand what is said and written to make sense of the human language. It is used to apply machine learning algorithms to text and speech.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Alt text](images/NLP.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS3m-UB936Eh"
      },
      "source": [
        "### What are the techniques used in NLP?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOdCnSf63Eov"
      },
      "source": [
        "NLP has primarily two aspects: natural language understanding (NLU) or natural language interpretation (NLI) (i.e. human to machine) and natural language generation (NLG) (i.e. machine to human). In simple words, one can say that NLG is inverse of NLU (broadly called as NLP). Natural language generation (NLG) is when software automatically transforms data into written narrative.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na9hVR4Z7u1M"
      },
      "source": [
        "### SYNTACTIC & SEMANTIC ANALYSIS\n",
        "Natural Language Processing tasks are primarily achieved by syntactic analysis and semantic analysis. There are many process involves in this process like:\n",
        "\n",
        "- Named entity recognition (NER) — determine the parts of a text that can be identified and categorized into preset groups, like names of people and objects.\n",
        "- Word sense disambiguation — give meaning to words based on their context\n",
        "- Natural language generation (NLG):— It involves using databases to derive semantic intentions and convert them into human language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICuUpdev8shb"
      },
      "source": [
        "### Programming\n",
        "\n",
        "In python, we have several libraries to work with text.\n",
        "\n",
        "- Scikit-learn, Keras, TensorFlow — has some text processing capabilities\n",
        "- NLTK — Natural language toolkit.\n",
        "- SpaCy — is an industrial strength NLP package with many practical tools in a nice API.\n",
        "\n",
        "Other libraries — TextBlob, gensim, Stanford CoreNLP, OpenNLP"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Alt text](images/normalization.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lbnM8TgHGdE"
      },
      "source": [
        "### Types:\n",
        "- Pattern based (like we find pattern of characters or strings to match)\n",
        "- AI based (we use DL, seq2seq or transformers) to see the context of language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VQOsftpLKPU"
      },
      "source": [
        "## Understand vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMwkvbzsLVWt",
        "outputId": "00583faa-e154-46ee-b65d-b059a6c1e3a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({'My': 1,\n",
              "         'name': 1,\n",
              "         'is': 2,\n",
              "         'hamsof,': 1,\n",
              "         'and': 1,\n",
              "         'meaning': 1,\n",
              "         'of': 1,\n",
              "         'hamsof': 1,\n",
              "         '...': 1})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## One simple way to process our english language is BOW\n",
        "\n",
        "from collections import Counter\n",
        "s = \"My name is hamsof, and meaning of hamsof is ...\"\n",
        "token_counter = Counter(s.split())\n",
        "\n",
        "token_counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0YwoJ2fMfyv"
      },
      "source": [
        "We can see there are many issues here like it is capturing hamsof and hamsof, differently, lets resolve it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0ZP_PUKNKZk"
      },
      "source": [
        "### Lets now talk about punctuation, stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEJWwQsbPW1M"
      },
      "source": [
        "Regular expressions are a way to improve vocabulary by spliting not only ' ' white spaces but also on ? or signs like stop. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOc4Ia81Nulx",
        "outputId": "6dbb6fcd-c444-4458-fb26-3bc1474738e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Natural',\n",
              " ' ',\n",
              " 'Language',\n",
              " ' ',\n",
              " 'Processing',\n",
              " ' ',\n",
              " 'is',\n",
              " ' ',\n",
              " 'so',\n",
              " ' ',\n",
              " 'awesome',\n",
              " ' ',\n",
              " \"isn't\",\n",
              " ' ',\n",
              " 'it']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "pattern = re.compile(r\"([-\\s.,;!?])+\")\n",
        "sentence = \"Natural Language Processing is so awesome, isn't it?\"\n",
        "tokens = pattern.split(sentence)\n",
        "tokens = [token for token in tokens if token not in '-\\t\\n.,;!?']\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHfrzqWYPtf4"
      },
      "source": [
        "But Regrex takes too much complexity to code to tokenize our sentence, lets move to some built in functionality:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VyKUi3TSwuZ"
      },
      "source": [
        "Some of the most commonly used libraries are spaCy and NLTK. We will mostly utilize the NLTK library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaPA3bMgUZdD"
      },
      "source": [
        "### NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfmBn_WTQCB8",
        "outputId": "f628fbdc-b27a-44db-bdb5-d40f21f6bbe2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'is',\n",
              " 'so',\n",
              " 'awesome',\n",
              " ',',\n",
              " 'is',\n",
              " \"n't\",\n",
              " 'it',\n",
              " '?']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "sentence = \"Natural Language Processing is so awesome, isn't it?\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE3Z7hZsTMRn"
      },
      "source": [
        "This was pretty clean but some times we need to remove words like is am are they dont provide such meaningful meanings lets explore them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh-BBateThrb"
      },
      "source": [
        "#### STOP words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YKFXVPITLc7",
        "outputId": "279501d7-13dd-4c28-ca8a-58c9f8b551f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natural', 'Language', 'Processing', 'is', 'so', 'awesome', ',', 'is', \"n't\", 'it', '?']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Natural', 'Language', 'Processing', 'awesome', ',', \"n't\", '?']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "sentence = \"Natural Language Processing is so awesome, isn't it?\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "print(tokens)\n",
        "\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lc1ROkvUJAB"
      },
      "source": [
        "Now you can compare the difference between these strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvV7SNISaLGp"
      },
      "source": [
        "### Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYVmNICeUPdF"
      },
      "source": [
        "Stemming and lemmatization are methods used by search engines and chatbots to analyze the meaning behind a word. Stemming uses the stem of the word, while lemmatization uses the context in which the word is being used."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Alt text](images/lemma%20or%20Stemma.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWQ20P0Ap_kT"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This indiscriminate cutting can be successful in some occasions, but not always, and that is why we affirm that this approach presents some limitations. Below we illustrate the method with examples in both English and Spanish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4MwGyhjqMLw"
      },
      "source": [
        "### Lemmatizing\n",
        "\n",
        "Lemmatization, on the other hand, takes into consideration the morphological analysis of the words. To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link the form back to its lemma. Again, you can see how it works with the same example words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTNyvHDoq4My"
      },
      "source": [
        "But sometimes stemming becomes costly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uoA0rdtUaTlY",
        "outputId": "a70c2627-b136-4c9d-cc49-d0bc6579e608"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'care'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "token = \"caring\"\n",
        "stemmer = PorterStemmer()\n",
        "stems = stemmer.stem(token)\n",
        "\n",
        "stems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlGMrLP9rnc0"
      },
      "source": [
        "And we can clearly see the difference with Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "z1EqHgV4rmyu",
        "outputId": "78957cf4-94b8-4fbc-e764-df9fbaa56488"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'caring'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('caring')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxvbDnBtz5PS"
      },
      "source": [
        "Now we are making our effort to see if reviews from IMDB is postitive or negative with the help og BOW and then training the dataset using Naive Bayes Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL6irBtY0AC0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Load dataset, we only use training data\n",
        "(X, y), _ = tf.keras.datasets.imdb.load_data()\n",
        "\n",
        "# Create a Bag-Of-Words Dataframe\n",
        "X = [Counter(x) for x in X[:5000]]\n",
        "y = y[:5000]\n",
        "\n",
        "X = pd.DataFrame(X).fillna(0).astype(int)\n",
        "\n",
        "# Split dataset into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True, random_state=1)\n",
        "\n",
        "# Instantiate and fit model on training data\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make a prediction and get accuracy\n",
        "prediction = clf.predict(X_test)\n",
        "accuracy = np.sum(y_test==prediction) / len(y_test)\n",
        "\n",
        "print(accuracy)\n",
        "# >>> 0.798"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
