## Stochastic Gradient Descent:
`SGD takes a random instance of the training data at each step and computes the gradient. This makes it much faster than BGD as it processes much less data at a time.`<br>
Stochastic Gradient Descent is implemented using numpy. The working of SGD is explained step by step using toy dataset and then the implementation is done on real world dataset.

### Packages Required:
1. numpy
2. pandas
3. matplotlib

Data Sets are attached, along with the Jupyter Notebook.
The Dataset "Book1.csv" is the dataset of toy example while "GPA.csv" is the real dataset.
<br>
